{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_Tianbo_Qiu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doublelockcup/Pathology-Images-Classification/blob/master/HW5_Tianbo_Qiu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "91pNIQrUV1X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "8485fd2e-307d-40f1-bf03-e38d2df54d12"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 48kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 4.1MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/12/5b/7196b11bca204cb6ca9000b5dc910e809081f224c73ef28e9991080e4e51/sacrebleu-1.3.1.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/c0/fb/1c7f9b3a71f64cdf86291cc645596f71746807bf2f72b3c1dd\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qze_KXFJWcg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf0ee58c-9e4e-42d6-9475-7e5243657f56"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XTDmaQ07Wif_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4a936150-66c1-446d-a5de-29bea54f839d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BvtAvM7HWmRl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp '/content/gdrive/My Drive/DeepLearning/datasets/spa.txt' 'spa.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gt2sy-HW6hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ]
    },
    {
      "metadata": {
        "id": "ibRM77wHXZDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Translate English to Spanish"
      ]
    },
    {
      "metadata": {
        "id": "lSnTC4vRWwkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spa = open('spa.txt', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyVOqI3xW9CH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8909acdd-89d4-4d77-c864-46e6a05e83be"
      },
      "cell_type": "code",
      "source": [
        "data = spa.read().splitlines()\n",
        "sentences_data = [tuple(d.split('\\t')) for d in data]\n",
        "\n",
        "print(len(sentences_data))\n",
        "print(sentences_data[:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118964\n",
            "[('Go.', 'Ve.'), ('Go.', 'Vete.'), ('Go.', 'Vaya.'), ('Go.', 'Váyase.'), ('Hi.', 'Hola.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yWen9LSyXhuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ab734d8-c031-49dd-8679-48900a452917"
      },
      "cell_type": "code",
      "source": [
        "def preprocess(s):\n",
        "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  s = s.strip()\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "# sample 3000 sentences as training set\n",
        "size = 3000\n",
        "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences_data[:size]]\n",
        "print('Original:', sentences_data[1500])\n",
        "print('Preprocessed:', sentences[1500])\n",
        "\n",
        "source_sentences, target_sentences = list(zip(*sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: ('We must go.', 'Nos debemos ir.')\n",
            "Preprocessed: ('<start> We must go . <end>', '<start> Nos debemos ir . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DH2uyLknZCrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Translator:\n",
        "  def __init__(self, source_sentences, target_sentences):\n",
        "    self.source_sentences = source_sentences\n",
        "    self.target_sentences = target_sentences\n",
        "    self.source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    self.source_tokenizer.fit_on_texts(source_sentences)\n",
        "    source_data = self.source_tokenizer.texts_to_sequences(source_sentences)\n",
        "    self.source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "    \n",
        "    self.target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    self.target_tokenizer.fit_on_texts(target_sentences)\n",
        "    target_data = self.target_tokenizer.texts_to_sequences(target_sentences)\n",
        "    self.target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
        "    \n",
        "    self.target_labels = np.zeros(self.target_data.shape)\n",
        "    self.target_labels[:, 0:self.target_data.shape[1]-1] = self.target_data[:,1:]\n",
        "    \n",
        "    self.source_vocab_size = len(self.source_tokenizer.word_index) + 1\n",
        "    self.target_vocab_size = len(self.target_tokenizer.word_index) + 1\n",
        "    self.batch_size = batch_size= 10\n",
        "    self.dataset = tf.data.Dataset.from_tensor_slices((self.source_data, self.target_data, self.target_labels)).batch(batch_size)\n",
        "    \n",
        "    \n",
        "\n",
        "  def decode(self, encoded_data):\n",
        "    for number in encoded_data:\n",
        "      if number != 0:\n",
        "        print(\"%d -> %s\" % (number, self.source_tokenizer.index_word[number]))\n",
        "        \n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, translator):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_size = 32\n",
        "    self.rnn_size = 64\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(translator.source_vocab_size, self.embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(self.rnn_size, return_sequences=True, return_state=True)\n",
        "  \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.rnn_size))\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, translator):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding_size = 32\n",
        "    self.rnn_size = 64\n",
        "    self.embedding = tf.keras.layers.Embedding(translator.target_vocab_size, self.embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(self.rnn_size, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(translator.target_vocab_size)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state\n",
        "  \n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "\n",
        "\n",
        "class Trans:\n",
        "  def __init__(self, source_sentences, target_sentences):\n",
        "    self.t = t = Translator(source_sentences, target_sentences)\n",
        "    self.encoder = Encoder(t)\n",
        "    self.decoder = Decoder(t)\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.batch_size= batch_size = t.batch_size\n",
        "   \n",
        "  def translate_s(self, s):\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "    #source_sentences = self.t.source_sentences\n",
        "    #target_sentences = self.t.target_sentences\n",
        "    source_data = self.t.source_data\n",
        "    #target_data = self.t.target_data\n",
        "    #target_labels = self.t.target_labels\n",
        "    source_tokenizer = self.t.source_tokenizer\n",
        "    target_tokenizer = self.t.target_tokenizer\n",
        "    pad_dim = source_data.shape[1]\n",
        "    input_sent = source_tokenizer.texts_to_sequences([s])\n",
        "    input_sent = tf.keras.preprocessing.sequence.pad_sequences(input_sent, maxlen=pad_dim, padding='post')\n",
        "    #return input_sent\n",
        "    #input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    #return input_sent\n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "\n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "\n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "\n",
        "    translation = ' '.join(out_words)    \n",
        "    return '<start>' + translation\n",
        "    \n",
        "\n",
        "    \n",
        "  \n",
        "    \n",
        "    \n",
        "    \n",
        "  def translate(self, idx=None):\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "    source_sentences = self.t.source_sentences\n",
        "    target_sentences = self.t.target_sentences\n",
        "    source_data = self.t.source_data\n",
        "    target_data = self.t.target_data\n",
        "    target_labels = self.t.target_labels\n",
        "    target_tokenizer = self.t.target_tokenizer\n",
        "    #sentences = list(zip(source_sentences, target_sentences))\n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(source_sentences))\n",
        "\n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    #print('debug---->', input_sent.shape)\n",
        "\n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "\n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "\n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "\n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "\n",
        "    translation = ' '.join(out_words)    \n",
        "    return source_sentences[idx], target_sentences[idx], translation\n",
        "  \n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(self, source_seq, target_seq, target_labels, initial_state):\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_output, encoder_state = self.encoder(source_seq, initial_state)\n",
        "      logits, decoder_state = self.decoder(target_seq, encoder_state)\n",
        "      loss = calc_loss(target_labels, logits)\n",
        "    variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss\n",
        "  \n",
        "  def train(self, EPOCHS=200):\n",
        "    batch_size = self.batch_size\n",
        "    dataset = self.t.dataset\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      en_initial_states = self.encoder.init_state(batch_size)\n",
        "      for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "        loss = self.train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "        elapsed = time.time() - start\n",
        "      \n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "        input_sent, target_sent, translation = self.translate()\n",
        "        print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phazruCBsEQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trans = Trans(source_sentences, target_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qB2Du3bwsL5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "550a4ec6-4861-4a28-e270-9bf9acb4f6ed"
      },
      "cell_type": "code",
      "source": [
        "# before trainings\n",
        "#trans.translate()\n",
        "#trans.translate_s('hello world ok ')\n",
        "##print(trans.t.source_data.shape[1])"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start>tropece sere labio mordio leseando chupa deje <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "metadata": {
        "id": "jL83BYd_OoeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#trans2 = Trans(target_sentences, source_sentences)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sfrnS_vYOxcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "93c21561-8112-4441-8767-9f1aabcb8f56"
      },
      "cell_type": "code",
      "source": [
        "#trans2.translate_s(trans.translate_s('hello world ok '))"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start>danced more all slow book deaf me snowed excited excited annoying ocd buy kissed loss resigned lion easter ? some'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "metadata": {
        "id": "dO5kVZbIPI8l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Translate English into Spanish**"
      ]
    },
    {
      "metadata": {
        "id": "emHvw0H8POcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trans = Trans(source_sentences, target_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FDv6Q_xwmdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        },
        "outputId": "a671f61f-d076-4483-dfe5-bbdaa27770d6"
      },
      "cell_type": "code",
      "source": [
        "trans.train(EPOCHS=300)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0, Loss 1.7975, Time 4.26 sec\n",
            "Input: <start> Be thorough . <end>\n",
            "Target: <start> Se exhaustivo . <end>\n",
            "Translation: ¿ . <end>\n",
            "\n",
            "Epoch #10, Loss 0.9446, Time 1.43 sec\n",
            "Input: <start> Bless you . <end>\n",
            "Target: <start> Jesus . <end>\n",
            "Translation: estamos . <end>\n",
            "\n",
            "Epoch #20, Loss 0.6840, Time 1.42 sec\n",
            "Input: <start> Are you okay ? <end>\n",
            "Target: <start> ¿ Estas bien ? <end>\n",
            "Translation: ¿ quien va ? <end>\n",
            "\n",
            "Epoch #30, Loss 0.5747, Time 1.41 sec\n",
            "Input: <start> I like them . <end>\n",
            "Target: <start> Me gustan . <end>\n",
            "Translation: me gusta el sushi . <end>\n",
            "\n",
            "Epoch #40, Loss 0.5060, Time 1.42 sec\n",
            "Input: <start> Now move on . <end>\n",
            "Target: <start> Ahora prosigue . <end>\n",
            "Translation: ven a echarme una mano . <end>\n",
            "\n",
            "Epoch #50, Loss 0.4450, Time 1.42 sec\n",
            "Input: <start> I m curious . <end>\n",
            "Target: <start> Tengo curiosidad . <end>\n",
            "Translation: tengo curiosidad . <end>\n",
            "\n",
            "Epoch #60, Loss 0.3929, Time 1.44 sec\n",
            "Input: <start> I m full . <end>\n",
            "Target: <start> Estoy llena . <end>\n",
            "Translation: estoy tan cansado ! <end>\n",
            "\n",
            "Epoch #70, Loss 0.3431, Time 1.41 sec\n",
            "Input: <start> I waited . <end>\n",
            "Target: <start> Espere . <end>\n",
            "Translation: dormi . <end>\n",
            "\n",
            "Epoch #80, Loss 0.2995, Time 1.44 sec\n",
            "Input: <start> Has she come ? <end>\n",
            "Target: <start> ¿ Ha venido ? <end>\n",
            "Translation: ¿ ha venido ? <end>\n",
            "\n",
            "Epoch #90, Loss 0.2848, Time 1.41 sec\n",
            "Input: <start> We know why . <end>\n",
            "Target: <start> Conocemos el porque . <end>\n",
            "Translation: conocemos el porque . <end>\n",
            "\n",
            "Epoch #100, Loss 0.2360, Time 1.43 sec\n",
            "Input: <start> We re cold . <end>\n",
            "Target: <start> Tenemos frio . <end>\n",
            "Translation: tenemos frio . <end>\n",
            "\n",
            "Epoch #110, Loss 0.1937, Time 1.63 sec\n",
            "Input: <start> It s chilly . <end>\n",
            "Target: <start> Hace fresco . <end>\n",
            "Translation: hace fresco . <end>\n",
            "\n",
            "Epoch #120, Loss 0.1851, Time 1.45 sec\n",
            "Input: <start> Smell this . <end>\n",
            "Target: <start> Huele esto . <end>\n",
            "Translation: huelan esto . <end>\n",
            "\n",
            "Epoch #130, Loss 0.1692, Time 1.40 sec\n",
            "Input: <start> Send him in . <end>\n",
            "Target: <start> Mandalo adentro . <end>\n",
            "Translation: mandelo dentro . <end>\n",
            "\n",
            "Epoch #140, Loss 0.1503, Time 1.42 sec\n",
            "Input: <start> Do I look OK ? <end>\n",
            "Target: <start> ¿ Me veo bien ? <end>\n",
            "Translation: ¿ me veo bien ? <end>\n",
            "\n",
            "Epoch #150, Loss 0.1328, Time 1.46 sec\n",
            "Input: <start> How annoying ! <end>\n",
            "Target: <start> Que rollo ! <end>\n",
            "Translation: que hincha pelotas ! <end>\n",
            "\n",
            "Epoch #160, Loss 0.1113, Time 1.42 sec\n",
            "Input: <start> I got a job . <end>\n",
            "Target: <start> Encontre un trabajo . <end>\n",
            "Translation: encontre un trabajo . <end>\n",
            "\n",
            "Epoch #170, Loss 0.1017, Time 1.43 sec\n",
            "Input: <start> I ll cook . <end>\n",
            "Target: <start> Yo cocinare . <end>\n",
            "Translation: yo cocinare . <end>\n",
            "\n",
            "Epoch #180, Loss 0.1104, Time 1.43 sec\n",
            "Input: <start> Let s relax . <end>\n",
            "Target: <start> Relajemonos . <end>\n",
            "Translation: relajemonos . <end>\n",
            "\n",
            "Epoch #190, Loss 0.0798, Time 1.42 sec\n",
            "Input: <start> Say cheese . <end>\n",
            "Target: <start> Sonria . <end>\n",
            "Translation: di guisqui . <end>\n",
            "\n",
            "Epoch #200, Loss 0.0749, Time 1.44 sec\n",
            "Input: <start> Follow him . <end>\n",
            "Target: <start> Siguele . <end>\n",
            "Translation: siguele . <end>\n",
            "\n",
            "Epoch #210, Loss 0.0898, Time 1.41 sec\n",
            "Input: <start> I m not here . <end>\n",
            "Target: <start> Yo no estoy aqui . <end>\n",
            "Translation: yo no estoy aqui . <end>\n",
            "\n",
            "Epoch #220, Loss 0.0804, Time 1.60 sec\n",
            "Input: <start> Return fire . <end>\n",
            "Target: <start> Responded a sus disparos . <end>\n",
            "Translation: responded a sus disparos . <end>\n",
            "\n",
            "Epoch #230, Loss 0.0804, Time 1.43 sec\n",
            "Input: <start> I m drowning . <end>\n",
            "Target: <start> Me estoy ahogando . <end>\n",
            "Translation: me estoy ahogando . <end>\n",
            "\n",
            "Epoch #240, Loss 0.0848, Time 1.43 sec\n",
            "Input: <start> Look ahead . <end>\n",
            "Target: <start> Mira hacia adelante . <end>\n",
            "Translation: mira hacia adelante . <end>\n",
            "\n",
            "Epoch #250, Loss 0.0693, Time 1.42 sec\n",
            "Input: <start> What s good ? <end>\n",
            "Target: <start> ¿ Que esta bien ? <end>\n",
            "Translation: ¿ que esta bueno ? <end>\n",
            "\n",
            "Epoch #260, Loss 0.0789, Time 1.44 sec\n",
            "Input: <start> Tom s alert . <end>\n",
            "Target: <start> Tom esta alerta . <end>\n",
            "Translation: tom esta alerta . <end>\n",
            "\n",
            "Epoch #270, Loss 0.1177, Time 1.63 sec\n",
            "Input: <start> I have a dog . <end>\n",
            "Target: <start> Tengo un perro . <end>\n",
            "Translation: tengo un perro . <end>\n",
            "\n",
            "Epoch #280, Loss 0.0715, Time 1.44 sec\n",
            "Input: <start> Go to bed . <end>\n",
            "Target: <start> Vete a la cama . <end>\n",
            "Translation: vete a la cama . <end>\n",
            "\n",
            "Epoch #290, Loss 0.0625, Time 1.44 sec\n",
            "Input: <start> I knew that . <end>\n",
            "Target: <start> Lo sabia . <end>\n",
            "Translation: yo sabia eso . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OKXAmeYE4grT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BLEU score**"
      ]
    },
    {
      "metadata": {
        "id": "0d-NHWNd3zQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(trans):\n",
        "  references, hypotheses = [], []\n",
        "  source_sentences = trans.t.source_sentences\n",
        "  for i in range(len(source_sentences)):\n",
        "    input_sent, target_sent, translation = trans.translate(i)\n",
        "    references.append(target_sent)\n",
        "    hypotheses.append('<start>' + translation)\n",
        "  results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "  print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9DLysvw8rvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ecb024a1-3b74-4d39-8fa8-d20305a67325"
      },
      "cell_type": "code",
      "source": [
        "eval(trans)"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=34.66402798501484, counts=[9246, 5958, 2959, 1071], totals=[13623, 10623, 7623, 4623], precisions=[67.87051310284079, 56.08585145439142, 38.81673881673882, 23.16677482154445], bp=0.8058872011091621, sys_len=13623, ref_len=16563)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8MEyIE_4mnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ]
    },
    {
      "metadata": {
        "id": "tQN5hpsR4oX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Translate Spanish into Engish**"
      ]
    },
    {
      "metadata": {
        "id": "OjkMycbc4x2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        },
        "outputId": "ed2752d2-b284-4f31-8037-95d5bca815bb"
      },
      "cell_type": "code",
      "source": [
        "trans2 = Trans(target_sentences, source_sentences)\n",
        "trans2.train(EPOCHS=300)"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0, Loss 2.3342, Time 4.02 sec\n",
            "Input: <start> Llovio . <end>\n",
            "Target: <start> It rained . <end>\n",
            "Translation: i i . <end>\n",
            "\n",
            "Epoch #10, Loss 0.8774, Time 1.47 sec\n",
            "Input: <start> Tiro la toalla . <end>\n",
            "Target: <start> He gave up . <end>\n",
            "Translation: i can t ? <end>\n",
            "\n",
            "Epoch #20, Loss 0.7008, Time 1.46 sec\n",
            "Input: <start> ¿ Estoy equivocada ? <end>\n",
            "Target: <start> Am I wrong ? <end>\n",
            "Translation: i m not fat . <end>\n",
            "\n",
            "Epoch #30, Loss 0.6190, Time 1.66 sec\n",
            "Input: <start> Soy escritor . <end>\n",
            "Target: <start> I m a writer . <end>\n",
            "Translation: i m a doctor . <end>\n",
            "\n",
            "Epoch #40, Loss 0.4996, Time 1.45 sec\n",
            "Input: <start> Tom es nuevo . <end>\n",
            "Target: <start> Tom is new . <end>\n",
            "Translation: tom s crazy . <end>\n",
            "\n",
            "Epoch #50, Loss 0.3881, Time 1.48 sec\n",
            "Input: <start> Estas loco ! <end>\n",
            "Target: <start> You re nuts ! <end>\n",
            "Translation: you re nuts ! <end>\n",
            "\n",
            "Epoch #60, Loss 0.3106, Time 1.45 sec\n",
            "Input: <start> ¿ Ha acabado Tom ? <end>\n",
            "Target: <start> Is Tom done ? <end>\n",
            "Translation: did tom come ? <end>\n",
            "\n",
            "Epoch #70, Loss 0.2181, Time 1.47 sec\n",
            "Input: <start> Me lastimas . <end>\n",
            "Target: <start> You hurt me . <end>\n",
            "Translation: you hurt me . <end>\n",
            "\n",
            "Epoch #80, Loss 0.1594, Time 1.67 sec\n",
            "Input: <start> Complaceme . <end>\n",
            "Target: <start> Humor me . <end>\n",
            "Translation: humor me . <end>\n",
            "\n",
            "Epoch #90, Loss 0.1221, Time 1.44 sec\n",
            "Input: <start> Tomas voto . <end>\n",
            "Target: <start> Tom voted . <end>\n",
            "Translation: tom is poor . <end>\n",
            "\n",
            "Epoch #100, Loss 0.1423, Time 1.69 sec\n",
            "Input: <start> ¿ Por que es eso ? <end>\n",
            "Target: <start> Why is that ? <end>\n",
            "Translation: why is that ? <end>\n",
            "\n",
            "Epoch #110, Loss 0.0605, Time 1.45 sec\n",
            "Input: <start> Confia en Tom . <end>\n",
            "Target: <start> Trust Tom . <end>\n",
            "Translation: trust tom . <end>\n",
            "\n",
            "Epoch #120, Loss 0.0459, Time 1.45 sec\n",
            "Input: <start> Tenta esto . <end>\n",
            "Target: <start> Feel this . <end>\n",
            "Translation: feel this . <end>\n",
            "\n",
            "Epoch #130, Loss 0.0390, Time 1.51 sec\n",
            "Input: <start> Venios . <end>\n",
            "Target: <start> Come with me . <end>\n",
            "Translation: come with me . <end>\n",
            "\n",
            "Epoch #140, Loss 0.0394, Time 1.45 sec\n",
            "Input: <start> Te vimos . <end>\n",
            "Target: <start> We saw you . <end>\n",
            "Translation: we saw you . <end>\n",
            "\n",
            "Epoch #150, Loss 0.0289, Time 1.46 sec\n",
            "Input: <start> El esta cansado . <end>\n",
            "Target: <start> He is tired . <end>\n",
            "Translation: he is tired . <end>\n",
            "\n",
            "Epoch #160, Loss 0.0260, Time 1.45 sec\n",
            "Input: <start> Veo un leon . <end>\n",
            "Target: <start> I see a lion . <end>\n",
            "Translation: i see a lion . <end>\n",
            "\n",
            "Epoch #170, Loss 0.0341, Time 1.45 sec\n",
            "Input: <start> Dejeme ir ! <end>\n",
            "Target: <start> Let me go ! <end>\n",
            "Translation: leave town . <end>\n",
            "\n",
            "Epoch #180, Loss 0.0505, Time 1.46 sec\n",
            "Input: <start> Tom hizo un guino . <end>\n",
            "Target: <start> Tom winked . <end>\n",
            "Translation: tom winked . <end>\n",
            "\n",
            "Epoch #190, Loss 0.0194, Time 1.46 sec\n",
            "Input: <start> Soy ciega . <end>\n",
            "Target: <start> I m blind . <end>\n",
            "Translation: i m blind . <end>\n",
            "\n",
            "Epoch #200, Loss 0.0256, Time 1.47 sec\n",
            "Input: <start> Vayamonos . <end>\n",
            "Target: <start> Let s leave . <end>\n",
            "Translation: let s leave . <end>\n",
            "\n",
            "Epoch #210, Loss 0.0328, Time 1.45 sec\n",
            "Input: <start> ¿ Estas lleno ? <end>\n",
            "Target: <start> Are you full ? <end>\n",
            "Translation: are you full ? <end>\n",
            "\n",
            "Epoch #220, Loss 0.0191, Time 1.44 sec\n",
            "Input: <start> ¿ Firmaste ? <end>\n",
            "Target: <start> Did you sign ? <end>\n",
            "Translation: did you sign ? <end>\n",
            "\n",
            "Epoch #230, Loss 0.0194, Time 1.48 sec\n",
            "Input: <start> Apagalo . <end>\n",
            "Target: <start> Turn it off . <end>\n",
            "Translation: turn it off . <end>\n",
            "\n",
            "Epoch #240, Loss 0.0310, Time 1.66 sec\n",
            "Input: <start> Soy todo oidos . <end>\n",
            "Target: <start> I m all ears . <end>\n",
            "Translation: i m all ears . <end>\n",
            "\n",
            "Epoch #250, Loss 0.0215, Time 1.46 sec\n",
            "Input: <start> ¿ Hemos terminado ? <end>\n",
            "Target: <start> Are we done ? <end>\n",
            "Translation: are we done ? <end>\n",
            "\n",
            "Epoch #260, Loss 0.0177, Time 1.44 sec\n",
            "Input: <start> Necesito una siesta . <end>\n",
            "Target: <start> I need a nap . <end>\n",
            "Translation: i need a nap . <end>\n",
            "\n",
            "Epoch #270, Loss 0.0171, Time 1.48 sec\n",
            "Input: <start> Ve a por ello . <end>\n",
            "Target: <start> Go for it . <end>\n",
            "Translation: go get it . <end>\n",
            "\n",
            "Epoch #280, Loss 0.0214, Time 1.46 sec\n",
            "Input: <start> Es una muneca . <end>\n",
            "Target: <start> It s a doll . <end>\n",
            "Translation: it s a doll . <end>\n",
            "\n",
            "Epoch #290, Loss 0.0154, Time 1.66 sec\n",
            "Input: <start> Que incordio ! <end>\n",
            "Target: <start> How annoying ! <end>\n",
            "Translation: how annoying ! <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWkQRObg7at7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "78536826-8f0c-4bb6-8803-a3f1f9c93965"
      },
      "cell_type": "code",
      "source": [
        "eval(trans2)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=46.29991332860898, counts=[10759, 7597, 4599, 1916], totals=[14200, 11200, 8200, 5200], precisions=[75.76760563380282, 67.83035714285714, 56.08536585365854, 36.84615384615385], bp=0.8110410418138186, sys_len=14200, ref_len=17174)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dILVEX2d9fIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3"
      ]
    },
    {
      "metadata": {
        "id": "mszWvMSoAFsY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Back-translate"
      ]
    },
    {
      "metadata": {
        "id": "7dNT2DhpVzEl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def back_translate(trans, trans2):\n",
        "  references, hypotheses = [], []\n",
        "  source_sentences = trans.t.source_sentences\n",
        "  for s in source_sentences[:1000]:\n",
        "    references.append(s)\n",
        "    hypotheses.append(trans2.translate_s(trans.translate_s(s)))\n",
        "  return references, hypotheses\n",
        "\n",
        "references, hypotheses = back_translate(trans, trans2)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKqt8d4lXAQ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1750
        },
        "outputId": "ed99d33b-b856-4d76-b9a2-af5209735040"
      },
      "cell_type": "code",
      "source": [
        "list(zip(references, hypotheses))[100:200]"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start> Go away ! <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away ! <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away ! <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away ! <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away ! <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go away . <end>', '<start>keep it up ! <end>'),\n",
              " ('<start> Go home . <end>', '<start>stay close . <end>'),\n",
              " ('<start> Go slow . <end>', '<start>look help . <end>'),\n",
              " ('<start> Goodbye ! <end>', '<start>do come in ! <end>'),\n",
              " ('<start> Goodbye ! <end>', '<start>do come in ! <end>'),\n",
              " ('<start> Goodbye ! <end>', '<start>do come in ! <end>'),\n",
              " ('<start> Hang on ! <end>', '<start>anyone home . <end>'),\n",
              " ('<start> Hang on ! <end>', '<start>anyone home . <end>'),\n",
              " ('<start> Hang on ! <end>', '<start>anyone home . <end>'),\n",
              " ('<start> Hang on . <end>', '<start>we survived . <end>'),\n",
              " ('<start> He came . <end>', '<start>no one came . <end>'),\n",
              " ('<start> He quit . <end>', '<start>i am ready . <end>'),\n",
              " ('<start> Help me ! <end>', '<start>take these . <end>'),\n",
              " ('<start> Help me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Help me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Help me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Help us . <end>', '<start>take these . <end>'),\n",
              " ('<start> Hit Tom . <end>', '<start>tom is dead . <end>'),\n",
              " ('<start> Hold it ! <end>', '<start>take these . <end>'),\n",
              " ('<start> Hold on . <end>', '<start>we survived . <end>'),\n",
              " ('<start> Hold on . <end>', '<start>we survived . <end>'),\n",
              " ('<start> Hold on . <end>', '<start>we survived . <end>'),\n",
              " ('<start> Hug Tom . <end>', '<start>tom is dead . <end>'),\n",
              " ('<start> I agree . <end>', '<start>i agree . <end>'),\n",
              " ('<start> I agree . <end>', '<start>i agree . <end>'),\n",
              " ('<start> I bowed . <end>', '<start>let me that . <end>'),\n",
              " ('<start> I moved . <end>', '<start>i moved . <end>'),\n",
              " ('<start> I moved . <end>', '<start>i moved . <end>'),\n",
              " ('<start> I moved . <end>', '<start>i moved . <end>'),\n",
              " ('<start> I moved . <end>', '<start>i moved . <end>'),\n",
              " ('<start> I slept . <end>', '<start>take these . <end>'),\n",
              " ('<start> I tried . <end>', '<start>that works . <end>'),\n",
              " ('<start> I ll go . <end>', '<start>take these . <end>'),\n",
              " ('<start> I m Tom . <end>', '<start>that tom okay . <end>'),\n",
              " ('<start> I m fat . <end>', '<start>let s go in . <end>'),\n",
              " ('<start> I m fat . <end>', '<start>let s go in . <end>'),\n",
              " ('<start> I m fit . <end>', '<start>have a drink . <end>'),\n",
              " ('<start> I m hit ! <end>', '<start>stop that . <end>'),\n",
              " ('<start> I m old . <end>', '<start>good works . <end>'),\n",
              " ('<start> I m shy . <end>', '<start>drive on . <end>'),\n",
              " ('<start> I m wet . <end>', '<start>i m shy . <end>'),\n",
              " ('<start> It s OK . <end>', '<start>she tried . <end>'),\n",
              " ('<start> It s me ! <end>', '<start>i use this . <end>'),\n",
              " ('<start> It s me . <end>', '<start>i use this . <end>'),\n",
              " ('<start> Join us . <end>', '<start>go on . <end>'),\n",
              " ('<start> Join us . <end>', '<start>go on . <end>'),\n",
              " ('<start> Keep it . <end>', '<start>take these . <end>'),\n",
              " ('<start> Me , too . <end>', '<start>sleep tight . <end>'),\n",
              " ('<start> Open up . <end>', '<start>take these . <end>'),\n",
              " ('<start> Perfect ! <end>', '<start>keep moving ! <end>'),\n",
              " ('<start> See you . <end>', '<start>come on down . <end>'),\n",
              " ('<start> Show me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Show me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Show me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Shut up ! <end>', '<start>me t t go . <end>'),\n",
              " ('<start> Shut up ! <end>', '<start>me t t go . <end>'),\n",
              " ('<start> Skip it . <end>', '<start>take these . <end>'),\n",
              " ('<start> So long . <end>', '<start>look away . <end>'),\n",
              " ('<start> So long . <end>', '<start>look away . <end>'),\n",
              " ('<start> Stop it . <end>', '<start>take these . <end>'),\n",
              " ('<start> Stop it . <end>', '<start>take these . <end>'),\n",
              " ('<start> Take it . <end>', '<start>take these . <end>'),\n",
              " ('<start> Tell me . <end>', '<start>take these . <end>'),\n",
              " ('<start> Tom ate . <end>', '<start>i can t walk . <end>'),\n",
              " ('<start> Tom ran . <end>', '<start>he ran . <end>'),\n",
              " ('<start> Tom won . <end>', '<start>say an confused . <end>'),\n",
              " ('<start> Wait up . <end>', '<start>stop here . <end>'),\n",
              " ('<start> Wake up ! <end>', '<start>keep moving ! <end>'),\n",
              " ('<start> Wake up ! <end>', '<start>keep moving ! <end>'),\n",
              " ('<start> Wake up ! <end>', '<start>keep moving ! <end>'),\n",
              " ('<start> Wake up . <end>', '<start>keep moving ! <end>'),\n",
              " ('<start> Wash up . <end>', '<start>we buy cds . <end>'),\n",
              " ('<start> We care . <end>', '<start>nobody a loss ! <end>'),\n",
              " ('<start> We know . <end>', '<start>we know you . <end>'),\n",
              " ('<start> We lost . <end>', '<start>take these . <end>'),\n",
              " ('<start> Welcome . <end>', '<start>take these . <end>'),\n",
              " ('<start> Welcome . <end>', '<start>take these . <end>'),\n",
              " ('<start> Who ate ? <end>', '<start>we can t walk . <end>'),\n",
              " ('<start> Who ran ? <end>', '<start>who cares ? <end>'),\n",
              " ('<start> Who ran ? <end>', '<start>who cares ? <end>'),\n",
              " ('<start> Who won ? <end>', '<start>who cares ? <end>'),\n",
              " ('<start> Who won ? <end>', '<start>who cares ? <end>'),\n",
              " ('<start> Why not ? <end>', '<start>who cook ? <end>'),\n",
              " ('<start> You run . <end>', '<start>take these . <end>'),\n",
              " ('<start> You won . <end>', '<start>go on in . <end>'),\n",
              " ('<start> Am I fat ? <end>', '<start>i m eating . <end>'),\n",
              " ('<start> Ask them . <end>', '<start>take these . <end>'),\n",
              " ('<start> Ask them . <end>', '<start>take these . <end>'),\n",
              " ('<start> Back off ! <end>', '<start>take these . <end>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "metadata": {
        "id": "cJPxPJKKXqga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b25e8e21-f3f2-4ab7-f60d-9c5d7bc14823"
      },
      "cell_type": "code",
      "source": [
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print('back-translate', results)"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "back-translate BLEU(score=6.501488223660218, counts=[2018, 943, 83, 14], totals=[4526, 3526, 2526, 1526], precisions=[44.5868316394167, 26.74418604651163, 3.285827395091053, 0.9174311926605505], bp=0.8396517914546833, sys_len=4526, ref_len=5317)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBKCWM4pa9CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "8b500c5c-7993-4a38-eb79-c36736572ce5"
      },
      "cell_type": "code",
      "source": [
        "t1 = Translator(source_sentences, target_sentences)\n",
        "print(t1.source_data[1000])\n",
        "print(t1.target_data[1000])\n",
        "\n",
        "print(t1.target_labels[200])\n",
        "print(t1.source_vocab_size)\n",
        "print(t1.target_vocab_size)\n",
        "\n",
        "t1.decode(t1.source_data[1000])\n",
        "\n",
        "ex_sentence = tf.expand_dims(t1.source_data[1000], axis=0)\n",
        "ex_translation = tf.expand_dims(t1.target_data[1000], axis=0)\n",
        "ex_labels = tf.expand_dims(t1.target_labels[1000], axis=0)\n",
        "print(ex_sentence)\n",
        "print(ex_translation)\n",
        "print(ex_labels)\n",
        "\n",
        "encoder = Encoder(t1)\n",
        "hidden_state = encoder.init_state(1)\n",
        "print(hidden_state.shape)\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)\n",
        "\n",
        "decoder = Decoder(t1)\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  13  12  17 432   3   2   0]\n",
            "[  1  11   9 505   3   2   0   0   0   0   0]\n",
            "[810.   3.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "918\n",
            "1909\n",
            "1 -> <start>\n",
            "13 -> he\n",
            "12 -> is\n",
            "17 -> a\n",
            "432 -> dj\n",
            "3 -> .\n",
            "2 -> <end>\n",
            "tf.Tensor([[  1  13  12  17 432   3   2   0]], shape=(1, 8), dtype=int32)\n",
            "tf.Tensor([[  1  11   9 505   3   2   0   0   0   0   0]], shape=(1, 11), dtype=int32)\n",
            "tf.Tensor([[ 11.   9. 505.   3.   2.   0.   0.   0.   0.   0.   0.]], shape=(1, 11), dtype=float64)\n",
            "(1, 64)\n",
            "(1, 8, 64)\n",
            "(1, 11, 1909)\n",
            "Loss tf.Tensor(3.4339483, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_RCzJa1kCz8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "4e6e741a-a44c-4145-a2b0-b76ab4629569"
      },
      "cell_type": "code",
      "source": [
        "t = trans.t\n",
        "(ssource, ttarget)=s = sentences[1000]\n",
        "print(s)\n",
        "ss_data = t.source_tokenizer.texts_to_sequences(ssource)\n",
        "ss_data = tf.keras.preprocessing.sequence.pad_sequences(ss_data, padding='post')\n",
        "print(len(ss_data))\n",
        "een = trans.encoder\n",
        "ddn = trans.decoder\n",
        "\n",
        "input_sent = ss_data\n",
        "input_send = tf.expand_dims(input_sent, axis=0)\n",
        "print(input_send)\n",
        "print(trans.t.source_data[1000])\n",
        "print(trans.t.source_sentences[1000])\n",
        "print(ssource)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> He is a DJ . <end>', '<start> El es DJ . <end>')\n",
            "26\n",
            "tf.Tensor(\n",
            "[[[  0]\n",
            "  [  8]\n",
            "  [ 21]\n",
            "  [ 17]\n",
            "  [  0]\n",
            "  [ 21]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  4]\n",
            "  [  8]\n",
            "  [  0]\n",
            "  [ 17]\n",
            "  [  0]\n",
            "  [354]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  3]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [  0]\n",
            "  [354]\n",
            "  [  0]]], shape=(1, 26, 1), dtype=int32)\n",
            "[  1  13  12  17 432   3   2   0]\n",
            "<start> He is a DJ . <end>\n",
            "<start> He is a DJ . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oI-R6pu3HLLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "3a3acb96-ec2a-4682-94b5-6b81d5d18625"
      },
      "cell_type": "code",
      "source": [
        "print(ssource)\n",
        "print(trans.t.source_sentences[1000])\n",
        "print(trans.t.source_data[1000])\n",
        "\n",
        "ssdata = trans.t.source_tokenizer.texts_to_sequences([ssource])\n",
        "ssdata = tf.keras.preprocessing.sequence.pad_sequences(ssdata, maxlen=8, padding='post')\n",
        "print(ssdata[0])\n",
        "\n",
        "np.array(ssdata)\n",
        "np.expand_dims(ssdata, axis=0)"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> He is a DJ . <end>\n",
            "<start> He is a DJ . <end>\n",
            "[  1  13  12  17 432   3   2   0]\n",
            "[  1  13  12  17 432   3   2   0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[  1,  13,  12,  17, 432,   3,   2,   0]]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "metadata": {
        "id": "9lzvpzlsJJv4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "642218ab-f5a8-4b2a-cccf-9fde350eac89"
      },
      "cell_type": "code",
      "source": [
        "print(trans.t.source_data.shape)\n",
        "print(trans.t.target_data.shape)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3000, 8)\n",
            "(3000, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lx58KZ07HmRI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2931
        },
        "outputId": "24519b51-068d-4759-ff76-fb2cdd995211"
      },
      "cell_type": "code",
      "source": [
        "help(trans.t.source_tokenizer)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on Tokenizer in module keras_preprocessing.text object:\n",
            "\n",
            "class Tokenizer(builtins.object)\n",
            " |  Text tokenization utility class.\n",
            " |  \n",
            " |  This class allows to vectorize a text corpus, by turning each\n",
            " |  text into either a sequence of integers (each integer being the index\n",
            " |  of a token in a dictionary) or into a vector where the coefficient\n",
            " |  for each token could be binary, based on word count, based on tf-idf...\n",
            " |  \n",
            " |  # Arguments\n",
            " |      num_words: the maximum number of words to keep, based\n",
            " |          on word frequency. Only the most common `num_words-1` words will\n",
            " |          be kept.\n",
            " |      filters: a string where each element is a character that will be\n",
            " |          filtered from the texts. The default is all punctuation, plus\n",
            " |          tabs and line breaks, minus the `'` character.\n",
            " |      lower: boolean. Whether to convert the texts to lowercase.\n",
            " |      split: str. Separator for word splitting.\n",
            " |      char_level: if True, every character will be treated as a token.\n",
            " |      oov_token: if given, it will be added to word_index and used to\n",
            " |          replace out-of-vocabulary words during text_to_sequence calls\n",
            " |  \n",
            " |  By default, all punctuation is removed, turning the texts into\n",
            " |  space-separated sequences of words\n",
            " |  (words maybe include the `'` character). These sequences are then\n",
            " |  split into lists of tokens. They will then be indexed or vectorized.\n",
            " |  \n",
            " |  `0` is a reserved index that won't be assigned to any word.\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit_on_sequences(self, sequences)\n",
            " |      Updates internal vocabulary based on a list of sequences.\n",
            " |      \n",
            " |      Required before using `sequences_to_matrix`\n",
            " |      (if `fit_on_texts` was never called).\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequence.\n",
            " |              A \"sequence\" is a list of integer word indices.\n",
            " |  \n",
            " |  fit_on_texts(self, texts)\n",
            " |      Updates internal vocabulary based on a list of texts.\n",
            " |      \n",
            " |      In the case where texts contains lists,\n",
            " |      we assume each entry of the lists to be a token.\n",
            " |      \n",
            " |      Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: can be a list of strings,\n",
            " |              a generator of strings (for memory-efficiency),\n",
            " |              or a list of list of strings.\n",
            " |  \n",
            " |  get_config(self)\n",
            " |      Returns the tokenizer configuration as Python dictionary.\n",
            " |      The word count dictionaries used by the tokenizer get serialized\n",
            " |      into plain JSON, so that the configuration can be read by other\n",
            " |      projects.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Python dictionary with the tokenizer configuration.\n",
            " |  \n",
            " |  sequences_to_matrix(self, sequences, mode='binary')\n",
            " |      Converts a list of sequences into a Numpy matrix.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: list of sequences\n",
            " |              (a sequence is a list of integer word indices).\n",
            " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy matrix.\n",
            " |      \n",
            " |      # Raises\n",
            " |          ValueError: In case of invalid `mode` argument,\n",
            " |              or if the Tokenizer requires to be fit to sample data.\n",
            " |  \n",
            " |  sequences_to_texts(self, sequences)\n",
            " |      Transforms each sequence into a list of text.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequences (list of integers).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of texts (strings)\n",
            " |  \n",
            " |  sequences_to_texts_generator(self, sequences)\n",
            " |      Transforms each sequence in `sequences` to a list of texts(strings).\n",
            " |      \n",
            " |      Each sequence has to a list of integers.\n",
            " |      In other words, sequences should be a list of sequences\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          sequences: A list of sequences.\n",
            " |      \n",
            " |      # Yields\n",
            " |          Yields individual texts.\n",
            " |  \n",
            " |  texts_to_matrix(self, texts, mode='binary')\n",
            " |      Convert a list of texts to a Numpy matrix.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: list of strings.\n",
            " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
            " |      \n",
            " |      # Returns\n",
            " |          A Numpy matrix.\n",
            " |  \n",
            " |  texts_to_sequences(self, texts)\n",
            " |      Transforms each text in texts to a sequence of integers.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: A list of texts (strings).\n",
            " |      \n",
            " |      # Returns\n",
            " |          A list of sequences.\n",
            " |  \n",
            " |  texts_to_sequences_generator(self, texts)\n",
            " |      Transforms each text in `texts` to a sequence of integers.\n",
            " |      \n",
            " |      Each item in texts can also be a list,\n",
            " |      in which case we assume each item of that list to be a token.\n",
            " |      \n",
            " |      Only top `num_words-1` most frequent words will be taken into account.\n",
            " |      Only words known by the tokenizer will be taken into account.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          texts: A list of texts (strings).\n",
            " |      \n",
            " |      # Yields\n",
            " |          Yields individual sequences.\n",
            " |  \n",
            " |  to_json(self, **kwargs)\n",
            " |      Returns a JSON string containing the tokenizer configuration.\n",
            " |      To load a tokenizer from a JSON string, use\n",
            " |      `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n",
            " |      \n",
            " |      # Arguments\n",
            " |          **kwargs: Additional keyword arguments\n",
            " |              to be passed to `json.dumps()`.\n",
            " |      \n",
            " |      # Returns\n",
            " |          A JSON string containing the tokenizer configuration.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}