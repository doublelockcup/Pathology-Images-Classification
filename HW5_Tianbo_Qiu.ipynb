{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5_Tianbo_Qiu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doublelockcup/Pathology-Images-Classification/blob/master/HW5_Tianbo_Qiu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "91pNIQrUV1X5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "8485fd2e-307d-40f1-bf03-e38d2df54d12"
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 332.1MB 48kB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 419kB 4.1MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/12/5b/7196b11bca204cb6ca9000b5dc910e809081f224c73ef28e9991080e4e51/sacrebleu-1.3.1.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n",
            "Building wheels for collected packages: sacrebleu\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/c0/fb/1c7f9b3a71f64cdf86291cc645596f71746807bf2f72b3c1dd\n",
            "Successfully built sacrebleu\n",
            "Installing collected packages: sacrebleu\n",
            "Successfully installed sacrebleu-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qze_KXFJWcg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf0ee58c-9e4e-42d6-9475-7e5243657f56"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XTDmaQ07Wif_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4a936150-66c1-446d-a5de-29bea54f839d"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BvtAvM7HWmRl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp '/content/gdrive/My Drive/DeepLearning/datasets/spa.txt' 'spa.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gt2sy-HW6hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ]
    },
    {
      "metadata": {
        "id": "ibRM77wHXZDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Translate English to Spanish"
      ]
    },
    {
      "metadata": {
        "id": "lSnTC4vRWwkj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spa = open('spa.txt', 'r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AyVOqI3xW9CH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8909acdd-89d4-4d77-c864-46e6a05e83be"
      },
      "cell_type": "code",
      "source": [
        "data = spa.read().splitlines()\n",
        "sentences_data = [tuple(d.split('\\t')) for d in data]\n",
        "\n",
        "print(len(sentences_data))\n",
        "print(sentences_data[:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118964\n",
            "[('Go.', 'Ve.'), ('Go.', 'Vete.'), ('Go.', 'Vaya.'), ('Go.', 'Váyase.'), ('Hi.', 'Hola.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yWen9LSyXhuZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1ab734d8-c031-49dd-8679-48900a452917"
      },
      "cell_type": "code",
      "source": [
        "def preprocess(s):\n",
        "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  s = s.strip()\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "# sample 3000 sentences as training set\n",
        "size = 3000\n",
        "sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences_data[:size]]\n",
        "print('Original:', sentences_data[1500])\n",
        "print('Preprocessed:', sentences[1500])\n",
        "\n",
        "source_sentences, target_sentences = list(zip(*sentences))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: ('We must go.', 'Nos debemos ir.')\n",
            "Preprocessed: ('<start> We must go . <end>', '<start> Nos debemos ir . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DH2uyLknZCrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Translator:\n",
        "  def __init__(self, source_sentences, target_sentences):\n",
        "    self.source_sentences = source_sentences\n",
        "    self.target_sentences = target_sentences\n",
        "    self.source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    self.source_tokenizer.fit_on_texts(source_sentences)\n",
        "    source_data = self.source_tokenizer.texts_to_sequences(source_sentences)\n",
        "    self.source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "    \n",
        "    self.target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    self.target_tokenizer.fit_on_texts(target_sentences)\n",
        "    target_data = self.target_tokenizer.texts_to_sequences(target_sentences)\n",
        "    self.target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
        "    \n",
        "    self.target_labels = np.zeros(self.target_data.shape)\n",
        "    self.target_labels[:, 0:self.target_data.shape[1]-1] = self.target_data[:,1:]\n",
        "    \n",
        "    self.source_vocab_size = len(self.source_tokenizer.word_index) + 1\n",
        "    self.target_vocab_size = len(self.target_tokenizer.word_index) + 1\n",
        "    self.batch_size = batch_size= 10\n",
        "    self.dataset = tf.data.Dataset.from_tensor_slices((self.source_data, self.target_data, self.target_labels)).batch(batch_size)\n",
        "    \n",
        "    \n",
        "\n",
        "  def decode(self, encoded_data):\n",
        "    for number in encoded_data:\n",
        "      if number != 0:\n",
        "        print(\"%d -> %s\" % (number, self.source_tokenizer.index_word[number]))\n",
        "        \n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, translator):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding_size = 32\n",
        "    self.rnn_size = 64\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(translator.source_vocab_size, self.embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(self.rnn_size, return_sequences=True, return_state=True)\n",
        "  \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    return output, state\n",
        "  \n",
        "  def init_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.rnn_size))\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, translator):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding_size = 32\n",
        "    self.rnn_size = 64\n",
        "    self.embedding = tf.keras.layers.Embedding(translator.target_vocab_size, self.embedding_size)\n",
        "    self.gru = tf.keras.layers.GRU(self.rnn_size, return_sequences=True, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(translator.target_vocab_size)\n",
        "    \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state=hidden)\n",
        "    logits = self.dense(output)\n",
        "    return logits, state\n",
        "  \n",
        "\n",
        "def calc_loss(targets, logits):\n",
        "  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int64)\n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "\n",
        "\n",
        "class Trans:\n",
        "  def __init__(self, source_sentences, target_sentences):\n",
        "    self.t = t = Translator(source_sentences, target_sentences)\n",
        "    self.encoder = Encoder(t)\n",
        "    self.decoder = Decoder(t)\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    self.batch_size= batch_size = t.batch_size\n",
        "    \n",
        "  def translate(self, idx=None):\n",
        "    encoder = self.encoder\n",
        "    decoder = self.decoder\n",
        "    source_sentences = self.t.source_sentences\n",
        "    target_sentences = self.t.target_sentences\n",
        "    source_data = self.t.source_data\n",
        "    target_data = self.t.target_data\n",
        "    target_labels = self.t.target_labels\n",
        "    target_tokenizer = self.t.target_tokenizer\n",
        "    #sentences = list(zip(source_sentences, target_sentences))\n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(source_sentences))\n",
        "\n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "\n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "\n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "\n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "\n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "\n",
        "    translation = ' '.join(out_words)    \n",
        "    return source_sentences[idx], target_sentences[idx], translation\n",
        "  \n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(self, source_seq, target_seq, target_labels, initial_state):\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_output, encoder_state = self.encoder(source_seq, initial_state)\n",
        "      logits, decoder_state = self.decoder(target_seq, encoder_state)\n",
        "      loss = calc_loss(target_labels, logits)\n",
        "    variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return loss\n",
        "  \n",
        "  def train(self, EPOCHS=200):\n",
        "    batch_size = self.batch_size\n",
        "    dataset = self.t.dataset\n",
        "    for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      en_initial_states = self.encoder.init_state(batch_size)\n",
        "      for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "        loss = self.train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "        elapsed = time.time() - start\n",
        "      \n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "        input_sent, target_sent, translation = self.translate()\n",
        "        print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "phazruCBsEQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trans = Trans(source_sentences, target_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qB2Du3bwsL5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "20b0c203-c045-4ee5-f7cf-4b73fcea04ef"
      },
      "cell_type": "code",
      "source": [
        "# before trainings\n",
        "trans.translate()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<start> Can you read ? <end>',\n",
              " '<start> ¿ Puedes leer ? <end>',\n",
              " 'mayores deje mayores ricos carne olvidese llevar fantastico chino comprometida equivocado pensando pruebalo viejos resbalo ganaste herido veni ninguno sentaba')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "3FDv6Q_xwmdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "e2e6bb9a-18a9-4f52-bddc-43ae8e931d7a"
      },
      "cell_type": "code",
      "source": [
        "trans.train(EPOCHS=30)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0, Loss 1.8154, Time 3.95 sec\n",
            "Input: <start> Who died ? <end>\n",
            "Target: <start> ¿ Quien murio ? <end>\n",
            "Translation: . <end>\n",
            "\n",
            "Epoch #10, Loss 0.9399, Time 2.15 sec\n",
            "Input: <start> I am sick . <end>\n",
            "Target: <start> Estoy enferma . <end>\n",
            "Translation: me gusta el la perro . <end>\n",
            "\n",
            "Epoch #20, Loss 0.6965, Time 1.81 sec\n",
            "Input: <start> I warned Tom . <end>\n",
            "Target: <start> He avisado a Tom . <end>\n",
            "Translation: me gusta el tom . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OKXAmeYE4grT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BLEU score**"
      ]
    },
    {
      "metadata": {
        "id": "0d-NHWNd3zQE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def eval(trans):\n",
        "  references, hypotheses = [], []\n",
        "  source_sentences = trans.t.source_sentences\n",
        "  for i in range(len(source_sentences)):\n",
        "    input_sent, target_sent, translation = trans.translate(i)\n",
        "    references.append(target_sent)\n",
        "    hypotheses.append('<start>' + translation)\n",
        "  results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "  print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9DLysvw8rvI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fd1f74a9-0945-4e3d-d159-0b4237b57d14"
      },
      "cell_type": "code",
      "source": [
        "eval(trans)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=3.721688841899096, counts=[5996, 2708, 70, 25], totals=[15033, 12033, 9033, 6033], precisions=[39.885585046231625, 22.504778525720933, 0.7749363445145577, 0.4143875352229405], bp=0.9032317693094347, sys_len=15033, ref_len=16563)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r8MEyIE_4mnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ]
    },
    {
      "metadata": {
        "id": "tQN5hpsR4oX6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Translate from Spanish to Engish"
      ]
    },
    {
      "metadata": {
        "id": "OjkMycbc4x2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "7f929538-e84a-4906-ff23-f3f8c0937df6"
      },
      "cell_type": "code",
      "source": [
        "trans2 = Trans(target_sentences, source_sentences)\n",
        "trans2.train(EPOCHS=20)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #0, Loss 2.2373, Time 3.80 sec\n",
            "Input: <start> Esto es una cancion . <end>\n",
            "Target: <start> It s a song . <end>\n",
            "Translation: i i . <end>\n",
            "\n",
            "Epoch #10, Loss 1.0042, Time 1.40 sec\n",
            "Input: <start> Estamos en casa . <end>\n",
            "Target: <start> We re home . <end>\n",
            "Translation: you re a dog . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oWkQRObg7at7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6ab55360-b204-4044-ae28-6adee418ee19"
      },
      "cell_type": "code",
      "source": [
        "eval(trans2)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=3.396092637509502, counts=[6218, 2673, 65, 26], totals=[16700, 13700, 10700, 7700], precisions=[37.23353293413174, 19.51094890510949, 0.6074766355140186, 0.33766233766233766], bp=0.9720157863692712, sys_len=16700, ref_len=17174)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dILVEX2d9fIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3"
      ]
    },
    {
      "metadata": {
        "id": "mszWvMSoAFsY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Back-translate"
      ]
    },
    {
      "metadata": {
        "id": "M3XTbBL2BrJc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BackTrans:\n",
        "  def __init__(self, trans, trans2):\n",
        "    self.en2sp = trans\n",
        "    self.sp2en = trans2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCiNlY0g9dzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2dd39e2-f2b1-4545-a691-773b8162a6ee"
      },
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "metadata": {
        "id": "DBKCWM4pa9CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "8b500c5c-7993-4a38-eb79-c36736572ce5"
      },
      "cell_type": "code",
      "source": [
        "t1 = Translator(source_sentences, target_sentences)\n",
        "print(t1.source_data[1000])\n",
        "print(t1.target_data[1000])\n",
        "\n",
        "print(t1.target_labels[200])\n",
        "print(t1.source_vocab_size)\n",
        "print(t1.target_vocab_size)\n",
        "\n",
        "t1.decode(t1.source_data[1000])\n",
        "\n",
        "ex_sentence = tf.expand_dims(t1.source_data[1000], axis=0)\n",
        "ex_translation = tf.expand_dims(t1.target_data[1000], axis=0)\n",
        "ex_labels = tf.expand_dims(t1.target_labels[1000], axis=0)\n",
        "print(ex_sentence)\n",
        "print(ex_translation)\n",
        "print(ex_labels)\n",
        "\n",
        "encoder = Encoder(t1)\n",
        "hidden_state = encoder.init_state(1)\n",
        "print(hidden_state.shape)\n",
        "output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "print(output.shape)\n",
        "\n",
        "decoder = Decoder(t1)\n",
        "decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "print(decoder_output.shape)\n",
        "\n",
        "print(\"Loss\", calc_loss(ex_labels, decoder_output))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  13  12  17 432   3   2   0]\n",
            "[  1  11   9 505   3   2   0   0   0   0   0]\n",
            "[810.   3.   2.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "918\n",
            "1909\n",
            "1 -> <start>\n",
            "13 -> he\n",
            "12 -> is\n",
            "17 -> a\n",
            "432 -> dj\n",
            "3 -> .\n",
            "2 -> <end>\n",
            "tf.Tensor([[  1  13  12  17 432   3   2   0]], shape=(1, 8), dtype=int32)\n",
            "tf.Tensor([[  1  11   9 505   3   2   0   0   0   0   0]], shape=(1, 11), dtype=int32)\n",
            "tf.Tensor([[ 11.   9. 505.   3.   2.   0.   0.   0.   0.   0.   0.]], shape=(1, 11), dtype=float64)\n",
            "(1, 64)\n",
            "(1, 8, 64)\n",
            "(1, 11, 1909)\n",
            "Loss tf.Tensor(3.4339483, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XCSExzBo0R5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def translate(idx=None):\n",
        "  encoder = trans.encoder\n",
        "  decoder = trans.decoder\n",
        "  source_data = trans.t.source_data\n",
        "  target_data = trans.t.target_data\n",
        "  target_labels = trans.t.target_labels\n",
        "  target_tokenizer = trans.t.target_tokenizer\n",
        "  if idx == None: \n",
        "    idx = np.random.choice(len(sentences))\n",
        "\n",
        "  input_sent = source_data[idx]\n",
        "  input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "\n",
        "  hidden_state = encoder.init_state(batch_size=1)\n",
        "  output, hidden_state = encoder(input_sent, hidden_state)\n",
        "\n",
        "  decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "  out_words = []\n",
        "\n",
        "  decoder_state = hidden_state\n",
        "\n",
        "  while True:\n",
        "\n",
        "      decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "      decoder_input = tf.argmax(decoder_output, -1)\n",
        "      word_idx = decoder_input.numpy()[0][0]\n",
        "      # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "      # before the decoder is trained, just stop translating and return\n",
        "      # what we have)\n",
        "      if word_idx == 0: \n",
        "        out_words.append('<end>')\n",
        "      else:\n",
        "        out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "      if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "        break\n",
        "\n",
        "  translation = ' '.join(out_words)    \n",
        "  return sentences[idx][0], sentences[idx][1], translation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOOqPMmC0hPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "0ab9bd8f-ce92-4dce-c5ef-3ee78b36f233"
      },
      "cell_type": "code",
      "source": [
        "input_sent, target_sent, translation = translate()\n",
        "print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> Wait up . <end>\n",
            "Target: <start> Esperame despierto . <end>\n",
            "Translation: esperame despierto . <end>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}